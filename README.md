<h1 align="center" style="font-size:3em"><b>Enterprise Knowledge Chatbot</b></h1>
<h2 align="center"><b>(Open-Source RAG Architecture)</b></h2>

‚ú® Introduction
Welcome to the Enterprise Knowledge Chatbot project! This is an open-source implementation of a Retrieval-Augmented Generation (RAG) system designed to help employees quickly access and leverage your organization's internal knowledge base through natural language conversations.

Problem: Enterprises struggle with information silos, making it difficult for employees to find answers within vast internal document repositories.

Solution: This chatbot connects to your company's documents, understands your queries, and provides context-aware answers, powered by cutting-edge AI.

üöÄ  üõ†Ô∏è Key Features
Automated Knowledge Ingestion: Daily indexing of documents from MinIO using Airflow.
Semantic Search: Fast and accurate retrieval of relevant information via Qdrant vector database.
AI-Powered Answers: Context-aware responses generated by OpenAI's LLM.
Chat History: Persistent conversation context managed by PostgreSQL.
Performance Caching: Redis for faster responses and cost optimization.
Intuitive UI: Built with Streamlit for easy user interaction.
Robust Backend: FastAPI serves as the API gateway.*
‚öôÔ∏è Technology Stack
AI Models: OpenAI (Embeddings: text-embedding-3-small, LLM: GPT-4 Turbo)
Vector Database: Qdrant
Object Storage: MinIO
Databases: PostgreSQL (History), Redis (Cache)
Orchestration: Apache Airflow OR AWS Step Functions
API Backend: FastAPI
User Interface: Streamlit
Language: Python
Containerization: Docker, Docker Compose*
üöÄ Getting Started
To run this project locally, you'll need Docker and Docker Compose installed.

1. Clone the Repository
git clone https://github.com/MohdIbrahim1795/Enterprise-Knowledge-RAG.git
cd Enterprise-Knowledge-RAG

2. Prepare Environment Variables
Copy the example environment file:
    cp .env.example .env
    
3. Build and Run Services with Docker Compose
docker-compose up --build

4. Access the Chatbot UI
Once all services are running, open your web browser and navigate to:

http://localhost:8501

You should see the Streamlit interface ready for your queries!

üèóÔ∏è System Architecture
The system follows a modern microservices architecture with the following components:

1. **Document Storage Layer**:
   - MinIO serves as the document storage system
   - Organized with "source" and "processed" buckets

2. **Processing Layer**:
   - Airflow orchestrates the document processing pipeline
   - Custom processing logic extracts text from documents
   - OpenAI API generates embeddings

3. **Knowledge Base Layer**:
   - Qdrant vector database stores document embeddings
   - Optimized for semantic similarity search

4. **API Layer**:
   - FastAPI provides RESTful endpoints
   - Handles authentication, query processing, and response generation

5. **Presentation Layer**:
   - Streamlit delivers an intuitive chat interface
   - Displays results with proper formatting

6. **Persistence Layer**:
   - PostgreSQL stores conversation history
   - Redis caches frequent queries for performance

This layered architecture ensures scalability, maintainability, and extensibility.

üõ†Ô∏è Development Setup Notes
FastAPI: Runs on http://localhost:8000 (internal service name fastapi_app:8000).
Streamlit: Runs on http://localhost:8501.
Airflow: Access the Airflow UI at http://localhost:8080 (default user/pass: admin/admin).
MinIO: Access the MinIO console at http://localhost:9001 (credentials defined in .env).
Qdrant: Vector database runs internally on port 6333.

üìù Document Processing

The system offers **two orchestration options** for document processing:

## Option 1: Airflow (Default - Docker Compose)
The traditional approach using Apache Airflow for workflow orchestration:

Documents are processed through the following pipeline:
1. Files are uploaded to the MinIO "source" directory
2. Airflow DAG runs daily to process new documents
3. Documents are chunked into smaller segments
4. OpenAI embeddings are generated for each chunk
5. Vector embeddings are stored in Qdrant
6. Processed files are moved to the "processed" directory

## Option 2: AWS Step Functions (Serverless)
A modern, cloud-native approach using AWS Step Functions:

üìÑ **New serverless implementation available!** See [`aws-stepfunctions/`](./aws-stepfunctions/) directory.

**Benefits of Step Functions approach:**
- ‚úÖ **Zero Infrastructure Maintenance** - Fully serverless
- ‚úÖ **Cost Efficient** - Pay only for document processing (94% cost savings for typical workloads)
- ‚úÖ **Auto Scaling** - Handles variable document volumes automatically
- ‚úÖ **Parallel Processing** - 10x faster for document batches
- ‚úÖ **Built-in Monitoring** - Native CloudWatch integration + SNS notifications
- ‚úÖ **High Reliability** - 99.9% SLA with automatic retries

### Quick Start with Step Functions:
```bash
cd aws-stepfunctions
./deployment/deploy.sh
```

**Supported document formats:**
- PDF documents (with full text extraction)
- Future support planned for DOC/DOCX, TXT, and more

Both architectures ensure efficient storage and retrieval of knowledge from your documents.

üîß Troubleshooting

## Airflow-specific Issues (Docker Compose setup):

**PDF Processing Issues**:
- Ensure the Airflow container has all necessary system dependencies (libgl1-mesa-glx, poppler-utils)
- Check Airflow logs for specific processing errors
- For stubborn PDFs, try pre-processing them with OCR software

**Missing Documents**:
- Confirm documents are uploaded to MinIO source directory
- Check Airflow DAG execution logs
- Verify document format is supported

## Step Functions-specific Issues (AWS setup):

**Lambda Function Errors**:
- Check CloudWatch logs for detailed error information
- Verify IAM permissions for S3, Qdrant access
- Monitor Lambda timeout settings for large documents

**Step Functions Execution Failures**:
- Review execution history in AWS console
- Check retry policies and error handling
- Validate input parameters and state transitions

## General Issues (Both implementations):

**Vector Database Connection**:
- Verify Qdrant is running with `docker ps` (local) or network connectivity (AWS)
- Check the collection exists with proper parameters
- Ensure embedding dimensions match (1536 for OpenAI)

**API Connection Issues**:
- Verify all containers are running with `docker ps` (Docker setup)
- Check environment variables in .env file
- Inspect container logs with `docker logs <container_name>`
- For AWS setup, check CloudWatch logs and service connectivity

**OpenAI API Issues**:
- Verify API key is valid and has sufficient credits
- Check rate limits and usage quotas
- Monitor embedding generation costs

For more complex issues, inspect specific container logs (Docker) or CloudWatch logs (AWS).

